{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXf3r0ViyebG"
      },
      "source": [
        "# Bayesian Cosmology: Measuring H‚ÇÄ with MCMC\n",
        "\n",
        "AST 3414 - Spring 2026\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this lab, you'll use **Bayesian inference** and **Markov Chain Monte Carlo (MCMC)** to measure the Hubble constant from supernova data which we previously fit with $\\chi^2$.\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "1. **Bayesian inference** - Sample the full posterior distribution, and allows for flexibility in constructing a generative model for your data\n",
        "2. **Prior selection** - Choose and justify reasonable priors\n",
        "3. **Posterior analysis** - Interpret MCMC chains and corner plots\n",
        "\n",
        "---\n",
        "\n",
        "## Bayes' Theorem\n",
        "\n",
        "$$P(\\theta | D) = \\frac{P(D | \\theta) \\cdot P(\\theta)}{P(D)}$$\n",
        "\n",
        "**Translation**:\n",
        "$$\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Evidence}}$$\n",
        "\n",
        "For parameter estimation, we can ignore the evidence (normalization):\n",
        "\n",
        "$$P(\\theta | D) \\propto P(D | \\theta) \\cdot P(\\theta)$$\n",
        "\n",
        "**What we need to define**:\n",
        "1. **Likelihood** $P(D | \\theta)$: How probable is our data given parameters?\n",
        "2. **Prior** $P(\\theta)$: What do we know about parameters before seeing data?\n",
        "\n",
        "MCMC then samples from the posterior to give us the full probability distribution of our parameters!\n",
        "\n",
        "---\n",
        "\n",
        "# Notes about MCMC\n",
        "\n",
        "Previously, you explores a Bayesian solution to fit a straight line using a mixture model. As you add parameters to a model you encounter the \"curse of dimensionality.\" Consider how many model evaluations you woulld need to for an N-dimensional gride with 100 samples per dimension.\n",
        "\n",
        "In one dimension (one parameter), you have 100 points.\n",
        "\n",
        "In two dimensions, you have $100^2 = 10,000$ evaluations.\n",
        "\n",
        "In three dimensions, you have $100^3 = 1,000,000$ evaluations.\n",
        "\n",
        "In $N$ dimensions, you have $100^N$ evaluations, and as $N$ grows this quickly becomes untenable with a grid search. The idea that made Bayesian modeling possible and practicle is *Markov Chain Monte Carlo (MCMC)*. This approach efficiently draws random samples from a posterior distribution even in relatively high dimensions.\n",
        "\n",
        "Look at the simplest MCMC sampling algorithm, the *Metropolis-Hastings Sampler*. This procedure draws psuedo-random chains of points which, in the long-term limit, are a representative sample from the posterior. The procedure is very simple:\n",
        "\n",
        "1. Define a posterior $p(\\theta|D,I)$\n",
        "2. Define a proposal density $p(\\theta_{i+1}|\\theta_i)$, which must be a symmetric function, but otherwise is unconstrained (usually a Gaussian is chosen).\n",
        "3. Choose a starting point $\\theta_0$\n",
        "4. Repeat the following:\n",
        "   1. Given $\\theta_i$, draw a new $\\theta_{i+1}$\n",
        "   2. Compute the acceptance ratio, $a = \\frac{p(\\theta_{i+1}|D,I)}{p(\\theta_{i}|D,I)}$.\n",
        "   3. If $a\\geq1$, the proposal is more likely: accept the draw and add $\\theta_{i+1}$ to the chain.\n",
        "   4. If $a\\lt 1$, then accept the point with probabilbity $a$. This can be done by drawing a uniform random number $r$ and checking if $a\n",
        "   \\lt r$. If the point is accepted, add $\\theta_{i+1}$ to the chain. If not, then add $\\theta_i$ to the chain *again*.\n",
        "\n",
        "This works very well! Multiple \"walkers\" can explore parameter space in parallel, which is efficient for moderate-dimensional problems (~10-20 parameters). Rather than climbing to a peak (optimization), the idea is to map out the entire landscape (sampling).\n",
        "\n",
        "Modern MCMC algorithms, such as **emcee** (Foreman-Mackey et al. 2013) which we are using, have some improvements to ensure efficient scaling of the posterior, but are not much different.  But there are a few caveats you should be aware of.\n",
        "\n",
        "##1. The procedure is only correct in the long-term limit\n",
        "Somethimes the long-term limit is **very** long. You want to find \"stabilization\" of the MCMC chain, meaning that it has reached a statistical equilibrium. Here we will just look at the chains to make sure stabilization has been reached.\n",
        "\n",
        "##2. The size of the proposal distribution is **very** important\n",
        "If your proposal distribution is too small, it will take too long for your chain to move, and you have the danger of getting stuck in a local maximum. If your proposal distribution is too large, you will not be able to efficiently explore the sapace around a particular peak.\n",
        "\n",
        "In general, choosing an appropriate scale for the proposal distribution is one of the most difficult parts of using the MCMC procedure. More sophisticated methods (such as **emcee**) have built-in ways to estimate the appropriate size of the proposal distribution.\n",
        "\n",
        "##3. Fast stabilization can be helped by a good initialization\n",
        "To ensure that the MCMC stabilizes quickly, start with a value you know is relatively close to the maximum a posteriori value, or the peak in the posterior distribution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DABS9zgeyebJ"
      },
      "source": [
        "## Part 1: Setup and Data Loading\n",
        "\n",
        "We'll need:\n",
        "- `numpy`, `matplotlib`: Standard scientific Python\n",
        "- `scipy`: For numerical integration  \n",
        "- `emcee`: MCMC sampler\n",
        "- `corner`: Visualization of posterior distributions\n",
        "- `pandas`: Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOUyCYC8mOOS"
      },
      "outputs": [],
      "source": [
        "# Install packages (Colab doesn't have emcee or corner by default)\n",
        "!pip install -q emcee corner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxnZEniOyebJ"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.integrate import quad\n",
        "from scipy.stats import norm\n",
        "import emcee\n",
        "import corner\n",
        "import pandas as pd\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Plot settings\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"‚úì All packages loaded successfully!\")\n",
        "print(f\"‚úì emcee version: {emcee.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skSSuQeCyebK"
      },
      "source": [
        "### Load Supernova Data\n",
        "\n",
        "We'll use the Pantheon+SH0ES dataset which you can download from my github using the code below. As a reminder, the Pantheon+SH0ES contains ~1700 Type Ia supernovae with: redshift (z) and uncertainty (œÉ_z), and distance modulus (Œº) and uncertainty (œÉ_Œº).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCWTIxKZmDMF"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/profhewitt/ast3414/refs/heads/main/Pantheon%2BSH0ES.dat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDmFLxk9neNp"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('Pantheon+SH0ES.dat', sep='\\\\s+', comment='#')\n",
        "\n",
        "# Extract columns (adjust names as needed based on actual file)\n",
        "if 'zHD' in data.columns:\n",
        "    z_obs = data['zHD'].values\n",
        "    mu_obs = data['MU_SH0ES'].values\n",
        "    sigma_mu = data['MU_SH0ES_ERR_DIAG'].values\n",
        "    sigma_z = 0.001 * np.ones_like(z_obs)  # Typical uncertainty\n",
        "else:\n",
        "    # Generic column access\n",
        "    z_obs = data.iloc[:, 1].values\n",
        "    mu_obs = data.iloc[:, 4].values\n",
        "    sigma_mu = data.iloc[:, 5].values\n",
        "    sigma_z = 0.001 * np.ones_like(z_obs)\n",
        "\n",
        "print(f\"‚úì Loaded {len(z_obs)} real supernovae from Pantheon+SH0ES\")\n",
        "data_source = \"real\"\n",
        "\n",
        "# Summary\n",
        "print(f\"\\nData summary:\")\n",
        "print(f\"  Redshift range: {z_obs.min():.4f} to {z_obs.max():.4f}\")\n",
        "print(f\"  Distance modulus range: {mu_obs.min():.2f} to {mu_obs.max():.2f}\")\n",
        "print(f\"  Mean œÉ_z: {sigma_z.mean():.5f}\")\n",
        "print(f\"  Mean œÉ_Œº: {sigma_mu.mean():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOpYV9fDyebL"
      },
      "source": [
        "### Visualize the Data\n",
        "\n",
        "Let's look at our Hubble diagram and understand the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5spgQ7ylyebL"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Left: Hubble diagram\n",
        "ax1 = axes[0]\n",
        "ax1.errorbar(z_obs, mu_obs, yerr=sigma_mu, xerr=sigma_z,\n",
        "             fmt='o', alpha=0.4, markersize=3, elinewidth=0.5,\n",
        "             label=f'{len(z_obs)} supernovae')\n",
        "ax1.set_xlabel('Redshift (z)', fontsize=14)\n",
        "ax1.set_ylabel('Distance Modulus (Œº)', fontsize=14)\n",
        "ax1.set_title('Hubble Diagram', fontsize=16)\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Right: Uncertainty distributions\n",
        "ax2 = axes[1]\n",
        "ax2.hist(sigma_z * 1000, bins=30, alpha=0.6, label='œÉ_z (√ó10‚Åª¬≥)', color='blue')\n",
        "ax2.hist(sigma_mu, bins=30, alpha=0.6, label='œÉ_Œº', color='red')\n",
        "ax2.set_xlabel('Uncertainty', fontsize=14)\n",
        "ax2.set_ylabel('Count', fontsize=14)\n",
        "ax2.set_title('Error Distributions', fontsize=16)\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8PN2JqxyebL"
      },
      "source": [
        "---\n",
        "## Part 2: The Model\n",
        "\n",
        "We'll fit a flat ŒõCDM cosmology with two parameters:\n",
        "- $H_0$: Hubble constant (km/s/Mpc)\n",
        "- $\\Omega_m$: Matter density parameter\n",
        "\n",
        "With the constraint: $\\Omega_\\Lambda = 1 -\\Omega_m$ (flat universe)\n",
        "\n",
        "### Luminosity Distance\n",
        "\n",
        "For a flat universe, the luminosity distance is:\n",
        "\n",
        "$$d_L(z) = \\frac{c(1+z)}{H_0} \\int_0^z \\frac{dz'}{E(z')}$$\n",
        "\n",
        "where:\n",
        "\n",
        "$$E(z) = \\sqrt{\\Omega_m(1+z)^3 + \\Omega_\\Lambda}$$\n",
        "\n",
        "and c = 299,792.458 km/s.\n",
        "\n",
        "### Distance Modulus\n",
        "\n",
        "$$\\mu(z) = 5\\log_{10}\\left(\\frac{d_L(z)}{10\\,\\text{pc}}\\right) = 5\\log_{10}(d_L) + 25$$\n",
        "\n",
        "where $d_L$ is in Mpc.\n",
        "\n",
        "Let's implement this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0mKQ4XHyebL"
      },
      "outputs": [],
      "source": [
        "# Speed of light in km/s\n",
        "c_light = 299792.458\n",
        "\n",
        "def E(z, Omega_m):\n",
        "    Omega_Lambda = 1 - Omega_m\n",
        "    return np.sqrt(Omega_m * (1 + z)**3 + Omega_Lambda)\n",
        "\n",
        "def luminosity_distance(z, H0, Omega_m):\n",
        "    z_array = np.atleast_1d(z)\n",
        "    d_L = np.zeros_like(z_array, dtype=float)\n",
        "\n",
        "    for i, zi in enumerate(z_array):\n",
        "        # Integrate 1/E(z) from 0 to zi\n",
        "        integrand = lambda zp: 1.0 / E(zp, Omega_m)\n",
        "        integral, _ = quad(integrand, 0, zi)\n",
        "\n",
        "        # Calculate luminosity distance\n",
        "        d_L[i] = (c_light / H0) * (1 + zi) * integral\n",
        "\n",
        "    return d_L if z.shape else float(d_L[0])\n",
        "\n",
        "def distance_modulus(z, H0, Omega_m):\n",
        "    d_L = luminosity_distance(z, H0, Omega_m)\n",
        "    return 5 * np.log10(d_L) + 25\n",
        "\n",
        "# Test the model\n",
        "test_z = np.array([0.1, 0.5, 1.0])\n",
        "test_H0 = 70\n",
        "test_Om = 0.3\n",
        "\n",
        "print(\"Model test:\")\n",
        "print(f\"  z = {test_z}\")\n",
        "print(f\"  H‚ÇÄ = {test_H0} km/s/Mpc, Œ©‚Çò = {test_Om}\")\n",
        "print(f\"  d_L = {luminosity_distance(test_z, test_H0, test_Om)} Mpc\")\n",
        "print(f\"  Œº = {distance_modulus(test_z, test_H0, test_Om)}\")\n",
        "print(\"\\n‚úì Model implementation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05ZdGKo7yebL"
      },
      "source": [
        "---\n",
        "## Part 3: Bayesian Inference Setup\n",
        "\n",
        "Now we define the components of Bayes' theorem.\n",
        "\n",
        "### 1. Prior Distribution P(Œ∏)\n",
        "\n",
        "We'll use **flat (uniform) priors** over physically reasonable ranges:\n",
        "\n",
        "- $H_0$: 50 to 90 km/s/Mpc\n",
        "  - Lower than 50 is inconsistent with local measurements\n",
        "  - Higher than 90 is inconsistent with CMB and structure formation\n",
        "\n",
        "- $\\Omega_m$: 0.1 to 0.5\n",
        "  - Lower than 0.1 has too little matter (conflicts with structure formation)\n",
        "  - Higher than 0.5 conflicts with CMB, would give $\\Omega_\\Lambda < 0.5$\n",
        "\n",
        "These are *weakly informative* priors - broad enough to not bias results, but exclude clearly unphysical values.\n",
        "\n",
        "### 2. Likelihood Function P(D|Œ∏)\n",
        "\n",
        "For a single supernova with measurements $(z_i^{\\text{obs}}, \\mu_i^{\\text{obs}})$ and uncertainties $(\\sigma_{z,i}, \\sigma_{\\mu,i})$:\n",
        "\n",
        "**Method**: We marginalize over the true redshift $z_i^{\\text{true}}$:\n",
        "\n",
        "$$P(\\mu_i^{\\text{obs}}, z_i^{\\text{obs}} | \\theta) = \\int P(\\mu_i^{\\text{obs}} | z_i^{\\text{true}}, \\theta) \\cdot P(z_i^{\\text{obs}} | z_i^{\\text{true}}) \\cdot P(z_i^{\\text{true}}) dz_i^{\\text{true}}$$\n",
        "\n",
        "**Assumptions**:\n",
        "1. Gaussian errors: $P(z_i^{\\text{obs}} | z_i^{\\text{true}}) = \\mathcal{N}(z_i^{\\text{true}}, \\sigma_{z,i}^2)$\n",
        "2. Gaussian errors: $P(\\mu_i^{\\text{obs}} | z_i^{\\text{true}}, \\theta) = \\mathcal{N}(\\mu_{\\text{model}}(z_i^{\\text{true}}, \\theta), \\sigma_{\\mu,i}^2)$\n",
        "3. Flat prior on $z_i^{\\text{true}}$ (uninformative)\n",
        "\n",
        "**In practice**: We'll numerically integrate over possible true redshifts near each observed redshift.\n",
        "\n",
        "**Total likelihood**: Product over all supernovae (assuming independence):\n",
        "\n",
        "$$\\mathcal{L}(\\theta) = \\prod_{i=1}^{N} P(\\mu_i^{\\text{obs}}, z_i^{\\text{obs}} | \\theta)$$\n",
        "\n",
        "We work with **log-likelihood** for numerical stability:\n",
        "\n",
        "$$\\ln \\mathcal{L}(\\theta) = \\sum_{i=1}^{N} \\ln P(\\mu_i^{\\text{obs}}, z_i^{\\text{obs}} | \\theta)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdOzuSjvyebM"
      },
      "outputs": [],
      "source": [
        "def log_prior(theta):\n",
        "    \"\"\"\n",
        "    Log prior probability.\n",
        "\n",
        "    Flat priors over reasonable ranges:\n",
        "    - H0: [50, 90] km/s/Mpc\n",
        "    - Omega_m: [0.1, 0.5]\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    theta : array-like\n",
        "        [H0, Omega_m]\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    log_prior : float\n",
        "        Log prior probability (0 if inside prior range, -inf if outside)\n",
        "    \"\"\"\n",
        "    H0, Omega_m = theta\n",
        "\n",
        "    # Check if parameters are in valid range\n",
        "    if 50 < H0 < 90 and 0.1 < Omega_m < 0.5:\n",
        "        return 0.0  # Flat prior (log(1) = 0)\n",
        "    else:\n",
        "        return -np.inf  # Outside prior range\n",
        "\n",
        "def log_likelihood_single(z_obs_i, mu_obs_i, sigma_z_i, sigma_mu_i, H0, Omega_m):\n",
        "    \"\"\"\n",
        "    Log likelihood for a single supernova, marginalizing over true redshift.\n",
        "\n",
        "    We integrate over possible true redshifts z_true near z_obs.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    z_obs_i : float\n",
        "        Observed redshift\n",
        "    mu_obs_i : float\n",
        "        Observed distance modulus\n",
        "    sigma_z_i : float\n",
        "        Redshift uncertainty\n",
        "    sigma_mu_i : float\n",
        "        Distance modulus uncertainty\n",
        "    H0 : float\n",
        "        Hubble constant\n",
        "    Omega_m : float\n",
        "        Matter density parameter\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    log_L : float\n",
        "        Log likelihood for this supernova\n",
        "    \"\"\"\n",
        "    # Integration range: ¬±5œÉ around observed redshift\n",
        "    z_min = max(0.001, z_obs_i - 5 * sigma_z_i)  # Don't go below z=0\n",
        "    z_max = z_obs_i + 5 * sigma_z_i\n",
        "\n",
        "    # Grid for numerical integration\n",
        "    n_points = 20  # Trade-off between accuracy and speed\n",
        "    z_true_grid = np.linspace(z_min, z_max, n_points)\n",
        "    dz = z_true_grid[1] - z_true_grid[0]\n",
        "\n",
        "    # Compute integrand at each grid point\n",
        "    integrand = np.zeros(n_points)\n",
        "    for j, z_true in enumerate(z_true_grid):\n",
        "        # Model prediction at true redshift\n",
        "        mu_model = distance_modulus(z_true, H0, Omega_m)\n",
        "\n",
        "        # P(mu_obs | z_true, theta) - Gaussian\n",
        "        log_p_mu = -0.5 * ((mu_obs_i - mu_model) / sigma_mu_i)**2\n",
        "\n",
        "        # P(z_obs | z_true) - Gaussian\n",
        "        log_p_z = -0.5 * ((z_obs_i - z_true) / sigma_z_i)**2\n",
        "\n",
        "        # Combined probability (in log space, add; in linear space, multiply)\n",
        "        integrand[j] = np.exp(log_p_mu + log_p_z)\n",
        "\n",
        "    # Numerical integration (trapezoidal rule)\n",
        "    integral = np.trapezoid(integrand, dx=dz)\n",
        "\n",
        "    # Return log of integral (with numerical stability check)\n",
        "    if integral > 0:\n",
        "        return np.log(integral)\n",
        "    else:\n",
        "        return -np.inf\n",
        "\n",
        "def log_likelihood(theta, z_obs, mu_obs, sigma_z, sigma_mu):\n",
        "    H0, Omega_m = theta\n",
        "    # Sum log likelihoods for all supernovae\n",
        "    log_L = 0.0\n",
        "    for i in range(len(z_obs)):\n",
        "        log_L += log_likelihood_single(\n",
        "            z_obs[i], mu_obs[i], sigma_z[i], sigma_mu[i], H0, Omega_m\n",
        "        )\n",
        "    return log_L\n",
        "\n",
        "def log_probability(theta, z_obs, mu_obs, sigma_z, sigma_mu):\n",
        "    \"\"\"\n",
        "    Log posterior probability (up to normalization constant).\n",
        "\n",
        "    log P(theta | data) = log P(data | theta) + log P(theta) + const\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    theta : array-like\n",
        "        [H0, Omega_m]\n",
        "    z_obs, mu_obs, sigma_z, sigma_mu : arrays\n",
        "        Data and uncertainties\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    log_prob : float\n",
        "        Log posterior probability\n",
        "    \"\"\"\n",
        "    # Check prior first (fast rejection of bad parameters)\n",
        "    lp = log_prior(theta)\n",
        "    if not np.isfinite(lp):\n",
        "        return -np.inf\n",
        "\n",
        "    # Add log likelihood\n",
        "    ll = log_likelihood(theta, z_obs, mu_obs, sigma_z, sigma_mu)\n",
        "\n",
        "    return lp + ll\n",
        "\n",
        "# Test the functions\n",
        "test_theta = [70, 0.3]\n",
        "print(\"Testing Bayesian functions:\")\n",
        "print(f\"  Œ∏ = {test_theta}\")\n",
        "print(f\"  log_prior(Œ∏) = {log_prior(test_theta)}\")\n",
        "print(f\"  log_likelihood (first SN) = {log_likelihood_single(z_obs[0], mu_obs[0], sigma_z[0], sigma_mu[0], *test_theta):.3f}\")\n",
        "print(\"\\n‚úì Bayesian functions ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh00R6b4yebM"
      },
      "source": [
        "---\n",
        "## Part 4: Running MCMC with emcee\n",
        "\n",
        "Now we'll use emcee to sample from our posterior distribution. MCMC takes a while because each likelihood evaluation requires a numerical integration, and there are many data points and steps needed to explore the parameter range.\n",
        "\n",
        "### MCMC Parameters\n",
        "\n",
        "- **n_walkers**: Number of parallel walkers (chains)\n",
        "  - Rule of thumb: ‚â• 2 √ó n_parameters\n",
        "  - We'll use 32 walkers for 2 parameters\n",
        "\n",
        "- **n_steps**: Number of steps each walker takes\n",
        "  - Need enough to converge and sample posterior well\n",
        "  - We'll use 2000 steps (then check convergence)\n",
        "\n",
        "- **Initial positions**: Where walkers start\n",
        "  - Should be near expected values but with some spread\n",
        "  - We'll start near H‚ÇÄ=70, Œ©‚Çò=0.3 with small perturbations\n",
        "\n",
        "In fact, **this takes too long** for 1700 supernovae. I estimated it as about 20 hours. Instead try running with just 10 supernovae. The supernovae are sorted in terms of redshift, so choose only the last 10 data points, as those are most constraining, and let's see what we get."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For faster testing, use a subset of data\n",
        "# write code here to select only last 10 data points for:\n",
        "#    z_obs, mu_obs, sigma_z, sigma_mu\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zjudyeFl2eDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "n8f7RV5cyebM"
      },
      "outputs": [],
      "source": [
        "print(f\"Running MCMC with {len(z_obs)} supernovae...\")\n",
        "print(\"This will take a few minutes... or hours... or days ¬Ø\\_(„ÉÑ)_/¬Ø. \\n\")\n",
        "\n",
        "# MCMC setup\n",
        "n_dim = 2  # Number of parameters (H0, Omega_m)\n",
        "n_walkers = 16  # Number of walkers (should be >> n_dim)\n",
        "n_steps = 1000  # Number of steps per walker\n",
        "\n",
        "# Initial positions for walkers\n",
        "# Start near expected values with small random perturbations\n",
        "initial_H0 = 70\n",
        "initial_Om = 0.3\n",
        "pos_spread = 1e-4  # Small spread\n",
        "\n",
        "initial_pos = np.array([initial_H0, initial_Om]) + pos_spread * np.random.randn(n_walkers, n_dim)\n",
        "\n",
        "# Ensure all initial positions are within prior range\n",
        "for i in range(n_walkers):\n",
        "    while log_prior(initial_pos[i]) == -np.inf:\n",
        "        initial_pos[i] = np.array([initial_H0, initial_Om]) + pos_spread * np.random.randn(n_dim)\n",
        "\n",
        "print(f\"MCMC Configuration:\")\n",
        "print(f\"  Parameters: H‚ÇÄ, Œ©‚Çò\")\n",
        "print(f\"  Walkers: {n_walkers}\")\n",
        "print(f\"  Steps: {n_steps}\")\n",
        "print(f\"  Total samples: {n_walkers * n_steps}\")\n",
        "print(f\"  Initial position: H‚ÇÄ‚âà{initial_H0}, Œ©‚Çò‚âà{initial_Om}\")\n",
        "\n",
        "# Initialize sampler\n",
        "sampler = emcee.EnsembleSampler(\n",
        "    n_walkers, n_dim, log_probability,\n",
        "    args=(z_obs, mu_obs, sigma_z, sigma_mu)\n",
        ")\n",
        "\n",
        "# Run MCMC\n",
        "print(f\"\\nRunning MCMC...\")\n",
        "print(\"(This may take 5-10 minutes depending on data size)\")\n",
        "sampler.run_mcmc(initial_pos, n_steps, progress=True)\n",
        "\n",
        "print(\"\\n‚úì MCMC sampling complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc1Fe6COyebM"
      },
      "source": [
        "---\n",
        "## Part 5: Analyzing the Results\n",
        "\n",
        "Now let's examine our MCMC chains and extract parameter estimates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Z31G9QRyebM"
      },
      "outputs": [],
      "source": [
        "# Get the chain\n",
        "# Shape: (n_walkers, n_steps, n_dim)\n",
        "samples = sampler.get_chain()\n",
        "\n",
        "print(f\"Chain shape: {samples.shape}\")\n",
        "print(f\"  {samples.shape[0]} walkers\")\n",
        "print(f\"  {samples.shape[1]} steps\")\n",
        "print(f\"  {samples.shape[2]} parameters\")\n",
        "\n",
        "# Plot the chains to check convergence\n",
        "fig, axes = plt.subplots(2, figsize=(10, 7), sharex=True)\n",
        "\n",
        "labels = [\"H‚ÇÄ [km/s/Mpc]\", \"Œ©‚Çò\"]\n",
        "for i in range(n_dim):\n",
        "    ax = axes[i]\n",
        "    ax.plot(samples[:, :, i], \"k\", alpha=0.3)\n",
        "    ax.set_ylabel(labels[i], fontsize=14)\n",
        "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
        "\n",
        "axes[-1].set_xlabel(\"Step Number\", fontsize=14)\n",
        "fig.suptitle(\"MCMC Chain Traces\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Trace Plots:\")\n",
        "print(\"  - Each thin line is one walker\")\n",
        "print(\"  - Should see 'hairy caterpillar' pattern\")\n",
        "print(\"  - Early steps are 'burn-in' (exploring from initial position)\")\n",
        "print(\"  - Later steps sample the posterior\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPovp60RyebN"
      },
      "source": [
        "### Burn-in and Thinning\n",
        "\n",
        "Looking at the traces, the chains are not long enough (only 1,000 steps) to be able to tell when the \"burn-in\" is done from the autocorrelation. Instead inspect by eye and set the point at which it looks like the MCMC converged. Discard any early steps where the chains are clearly still moving to the center of the posterior distribution from the initial positions. Our final chains should just look like random walks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kxun7iYByebN"
      },
      "outputs": [],
      "source": [
        "burnin = #\n",
        "thin = 15\n",
        "\n",
        "# Get flattened chain (all walkers combined, after burn-in and thinning)\n",
        "flat_samples = sampler.get_chain(discard=burnin, thin=thin, flat=True)\n",
        "\n",
        "print(f\"\\nFinal sample:\")\n",
        "print(f\"  Shape: {flat_samples.shape}\")\n",
        "print(f\"  Total samples: {flat_samples.shape[0]}\")\n",
        "print(f\"  (After discarding {burnin} burn-in steps and thinning by {thin})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHDKEsUryebN"
      },
      "source": [
        "---\n",
        "## Part 6: Parameter Estimates\n",
        "\n",
        "Now we can extract parameter estimates from our posterior samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BH1SOFv2yebN"
      },
      "outputs": [],
      "source": [
        "# Extract parameters\n",
        "H0_samples = flat_samples[:, 0]\n",
        "Om_samples = flat_samples[:, 1]\n",
        "\n",
        "# Calculate statistics\n",
        "H0_median = np.median(H0_samples)\n",
        "H0_std = np.std(H0_samples)\n",
        "H0_16, H0_84 = np.percentile(H0_samples, [16, 84])\n",
        "\n",
        "Om_median = np.median(Om_samples)\n",
        "Om_std = np.std(Om_samples)\n",
        "Om_16, Om_84 = np.percentile(Om_samples, [16, 84])\n",
        "\n",
        "# Derived quantities\n",
        "OL_median = 1 - Om_median\n",
        "q0_median = 0.5 * Om_median - OL_median\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"BAYESIAN PARAMETER ESTIMATES\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nH‚ÇÄ = {H0_median:.2f} +{H0_84-H0_median:.2f} -{H0_median-H0_16:.2f} km/s/Mpc\")\n",
        "print(f\"    (median ¬± std: {H0_median:.2f} ¬± {H0_std:.2f})\")\n",
        "\n",
        "print(f\"\\nŒ©‚Çò = {Om_median:.3f} +{Om_84-Om_median:.3f} -{Om_median-Om_16:.3f}\")\n",
        "print(f\"    (median ¬± std: {Om_median:.3f} ¬± {Om_std:.3f})\")\n",
        "\n",
        "print(f\"\\nDerived quantities:\")\n",
        "print(f\"  Œ©_Œõ = {OL_median:.3f} (= 1 - Œ©‚Çò)\")\n",
        "print(f\"  q‚ÇÄ = {q0_median:.3f} (= Œ©‚Çò/2 - Œ©_Œõ)\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Compare with literature\n",
        "print(f\"\\nLiterature values:\")\n",
        "print(f\"  H‚ÇÄ ‚âà 67-74 km/s/Mpc (tension between measurements!)\")\n",
        "print(f\"  Œ©‚Çò ‚âà 0.31 (Planck 2018)\")\n",
        "print(f\"  Œ©_Œõ ‚âà 0.69\")\n",
        "print(f\"  q‚ÇÄ ‚âà -0.54\")\n",
        "\n",
        "if data_source == \"synthetic\":\n",
        "    print(f\"\\nTrue values (synthetic data):\")\n",
        "    print(f\"  H‚ÇÄ = 70 km/s/Mpc\")\n",
        "    print(f\"  Œ©‚Çò = 0.30\")\n",
        "    print(f\"\\n  Did we recover the true parameters? ‚úì\" if abs(H0_median - 70) < 2 else \"  Check convergence!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff3Jiee8yebN"
      },
      "source": [
        "### Corner Plot\n",
        "\n",
        "A **corner plot** shows the full posterior distribution:\n",
        "- Diagonal: 1D marginalized posteriors (histograms)\n",
        "- Off-diagonal: 2D joint posteriors (contours)\n",
        "- Shows correlations between parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNV6h9EoyebN"
      },
      "outputs": [],
      "source": [
        "# Create corner plot\n",
        "fig = corner.corner(\n",
        "    flat_samples,\n",
        "    labels=[\"$H_0$ [km/s/Mpc]\", \"$\\Omega_m$\"],\n",
        "    truths=[70, 0.3] if data_source == \"synthetic\" else None,\n",
        "    quantiles=[0.16, 0.5, 0.84],\n",
        "    show_titles=True,\n",
        "    title_kwargs={\"fontsize\": 12},\n",
        "    label_kwargs={\"fontsize\": 14}\n",
        ")\n",
        "\n",
        "if data_source == \"synthetic\":\n",
        "    fig.suptitle(\"Posterior Distribution (red lines = true values)\", fontsize=16, y=1.02)\n",
        "else:\n",
        "    fig.suptitle(\"Posterior Distribution\", fontsize=16, y=1.02)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Corner Plot Interpretation:\")\n",
        "print(\"  - Diagonal panels: 1D posterior for each parameter\")\n",
        "print(\"  - Off-diagonal: 2D joint posterior (contours show 1œÉ, 2œÉ levels)\")\n",
        "print(\"  - Are H‚ÇÄ and Œ©‚Çò correlated or independent?\")\n",
        "print(\"  - Are posteriors roughly Gaussian or skewed?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sad0EmZiyebN"
      },
      "source": [
        "---\n",
        "## Part 7: Model Comparison\n",
        "\n",
        "Let's visualize how well our model fits the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNcadPS-yebN"
      },
      "outputs": [],
      "source": [
        "# Plot data and model with posterior uncertainty\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Left: Full fit\n",
        "ax1 = axes[0]\n",
        "\n",
        "# Data\n",
        "ax1.errorbar(z_obs, mu_obs, yerr=sigma_mu, xerr=sigma_z,\n",
        "             fmt='o', alpha=0.4, markersize=3, elinewidth=0.5,\n",
        "             color='gray', label='Data')\n",
        "\n",
        "# Model curves from posterior samples\n",
        "z_model = np.linspace(0, max(z_obs), 300)\n",
        "\n",
        "# Plot several random samples from posterior (to show uncertainty)\n",
        "n_plot = 100\n",
        "indices = np.random.randint(len(flat_samples), size=n_plot)\n",
        "\n",
        "for idx in indices:\n",
        "    H0_sample, Om_sample = flat_samples[idx]\n",
        "    mu_model = distance_modulus(z_model, H0_sample, Om_sample)\n",
        "    ax1.plot(z_model, mu_model, 'b-', alpha=0.02, linewidth=0.5)\n",
        "\n",
        "# Best fit (median)\n",
        "mu_best = distance_modulus(z_model, H0_median, Om_median)\n",
        "ax1.plot(z_model, mu_best, 'r-', linewidth=2.5,\n",
        "         label=f'Best fit: H‚ÇÄ={H0_median:.1f}, Œ©‚Çò={Om_median:.2f}')\n",
        "\n",
        "ax1.set_xlabel('Redshift (z)', fontsize=14)\n",
        "ax1.set_ylabel('Distance Modulus (Œº)', fontsize=14)\n",
        "ax1.set_title('Model Fit to Data', fontsize=16)\n",
        "ax1.legend(fontsize=12)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Right: Residuals\n",
        "ax2 = axes[1]\n",
        "\n",
        "mu_pred = distance_modulus(z_obs, H0_median, Om_median)\n",
        "residuals = mu_obs - mu_pred\n",
        "\n",
        "ax2.errorbar(z_obs, residuals, yerr=sigma_mu,\n",
        "             fmt='o', alpha=0.5, markersize=3, elinewidth=0.5)\n",
        "ax2.axhline(0, color='red', linestyle='--', linewidth=2)\n",
        "ax2.set_xlabel('Redshift (z)', fontsize=14)\n",
        "ax2.set_ylabel('Residuals (Œº_obs - Œº_model)', fontsize=14)\n",
        "ax2.set_title('Fit Residuals', fontsize=16)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Repeat with 200 data points\n",
        "\n",
        "We can see that 10 data points does not cut it! Repeat this exercise again, but instead use the '''sampler''' data produced on 200 data points. You can download this from our class github using the code below."
      ],
      "metadata": {
        "id": "g1QkUMVk3mE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -p https://raw.githubusercontent.com/profhewitt/ast3414/refs/heads/main/emcee_200samples.pkl"
      ],
      "metadata": {
        "id": "Pvoynrdt3-YH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71fb9f49-b526-4ffd-d45f-e82f25c8125b"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-16 20:38:37--  https://raw.githubusercontent.com/profhewitt/ast3414/refs/heads/main/emcee_200samples.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 512141 (500K) [application/octet-stream]\n",
            "Saving to: ‚Äòraw.githubusercontent.com/profhewitt/ast3414/refs/heads/main/emcee_200samples.pkl‚Äô\n",
            "\n",
            "\r          raw.githu   0%[                    ]       0  --.-KB/s               \rraw.githubuserconte 100%[===================>] 500.14K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2026-02-16 20:38:38 (13.3 MB/s) - ‚Äòraw.githubusercontent.com/profhewitt/ast3414/refs/heads/main/emcee_200samples.pkl‚Äô saved [512141/512141]\n",
            "\n",
            "FINISHED --2026-02-16 20:38:38--\n",
            "Total wall clock time: 0.1s\n",
            "Downloaded: 1 files, 500K in 0.04s (13.3 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to load in Hewitt's MCMC run\n",
        "import pickle\n",
        "# Open file in binary write mode ('wb') and dump the object\n",
        "with open('emcee_200samples.pkl', 'rb') as outfile:\n",
        "    samples = pickle.load(outfile)"
      ],
      "metadata": {
        "id": "j1TTJ7HH4KR3"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ADD YOUR CODE HERE"
      ],
      "metadata": {
        "id": "pfYwg8xq4KcQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b3fa2b5-f839-4b6a-a7d6-837ef9b79af0"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 16, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJba_SO8yebO"
      },
      "source": [
        "---\n",
        "# Discussion Questions\n",
        "\n",
        "1. How senstive are the results to the number of data points? How much do you think your results would improve using all 1700 supernovae? Justify your speculation.\n",
        "\n",
        "2. How would you need to modify the code to fit a non-flat universe (include either curvature or $\\Omega_\\Lambda$ as a free parameter)? You do not have to successfully re-run your model with this modification, but describe below how it could be done, or edit a bit of code."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}